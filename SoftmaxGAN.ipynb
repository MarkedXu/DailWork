{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f92fd87a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "\n",
      "Namespace(b1=0.5, b2=0.999, batch_size=64, channels=1, img_size=28, latent_dim=100, lr=0.0002, n_cpu=8, n_epochs=200, sample_interval=400)\n",
      "---------------\n",
      "\n",
      "[Epoch 0/200] [Batch 0/938] [D loss: 4.846114] [G loss: 4.852489]\n",
      "[Epoch 0/200] [Batch 1/938] [D loss: 4.672896] [G loss: 4.872837]\n",
      "[Epoch 0/200] [Batch 2/938] [D loss: 4.547186] [G loss: 4.923515]\n",
      "[Epoch 0/200] [Batch 3/938] [D loss: 4.457734] [G loss: 4.991141]\n",
      "[Epoch 0/200] [Batch 4/938] [D loss: 4.413053] [G loss: 5.040720]\n",
      "[Epoch 0/200] [Batch 5/938] [D loss: 4.415408] [G loss: 5.037146]\n",
      "[Epoch 0/200] [Batch 6/938] [D loss: 4.438271] [G loss: 5.006638]\n",
      "[Epoch 0/200] [Batch 7/938] [D loss: 4.460515] [G loss: 4.984210]\n",
      "[Epoch 0/200] [Batch 8/938] [D loss: 4.465087] [G loss: 4.979560]\n",
      "[Epoch 0/200] [Batch 9/938] [D loss: 4.455987] [G loss: 4.987940]\n",
      "[Epoch 0/200] [Batch 10/938] [D loss: 4.448716] [G loss: 4.995169]\n",
      "[Epoch 0/200] [Batch 11/938] [D loss: 4.441894] [G loss: 5.002050]\n",
      "[Epoch 0/200] [Batch 12/938] [D loss: 4.444034] [G loss: 4.999313]\n",
      "[Epoch 0/200] [Batch 13/938] [D loss: 4.450670] [G loss: 4.992721]\n",
      "[Epoch 0/200] [Batch 14/938] [D loss: 4.452080] [G loss: 4.991122]\n",
      "[Epoch 0/200] [Batch 15/938] [D loss: 4.445919] [G loss: 4.997340]\n",
      "[Epoch 0/200] [Batch 16/938] [D loss: 4.445069] [G loss: 4.997963]\n",
      "[Epoch 0/200] [Batch 17/938] [D loss: 4.447253] [G loss: 4.995774]\n",
      "[Epoch 0/200] [Batch 18/938] [D loss: 4.452633] [G loss: 4.990644]\n",
      "[Epoch 0/200] [Batch 19/938] [D loss: 4.448684] [G loss: 4.994370]\n",
      "[Epoch 0/200] [Batch 20/938] [D loss: 4.444059] [G loss: 4.998893]\n",
      "[Epoch 0/200] [Batch 21/938] [D loss: 4.446453] [G loss: 4.996378]\n",
      "[Epoch 0/200] [Batch 22/938] [D loss: 4.448736] [G loss: 4.994389]\n",
      "[Epoch 0/200] [Batch 23/938] [D loss: 4.445803] [G loss: 4.997013]\n",
      "[Epoch 0/200] [Batch 24/938] [D loss: 4.449178] [G loss: 4.993768]\n",
      "[Epoch 0/200] [Batch 25/938] [D loss: 4.446315] [G loss: 4.996468]\n",
      "[Epoch 0/200] [Batch 26/938] [D loss: 4.447306] [G loss: 4.995528]\n",
      "[Epoch 0/200] [Batch 27/938] [D loss: 4.446805] [G loss: 4.996081]\n",
      "[Epoch 0/200] [Batch 28/938] [D loss: 4.447312] [G loss: 4.995370]\n",
      "[Epoch 0/200] [Batch 29/938] [D loss: 4.448455] [G loss: 4.994513]\n",
      "[Epoch 0/200] [Batch 30/938] [D loss: 4.445947] [G loss: 4.996871]\n",
      "[Epoch 0/200] [Batch 31/938] [D loss: 4.446626] [G loss: 4.996118]\n",
      "[Epoch 0/200] [Batch 32/938] [D loss: 4.447619] [G loss: 4.995160]\n",
      "[Epoch 0/200] [Batch 33/938] [D loss: 4.445364] [G loss: 4.997482]\n",
      "[Epoch 0/200] [Batch 34/938] [D loss: 4.450010] [G loss: 4.992888]\n",
      "[Epoch 0/200] [Batch 35/938] [D loss: 4.446376] [G loss: 4.996507]\n",
      "[Epoch 0/200] [Batch 36/938] [D loss: 4.446365] [G loss: 4.996561]\n",
      "[Epoch 0/200] [Batch 37/938] [D loss: 4.449145] [G loss: 4.993622]\n",
      "[Epoch 0/200] [Batch 38/938] [D loss: 4.445311] [G loss: 4.997465]\n",
      "[Epoch 0/200] [Batch 39/938] [D loss: 4.447019] [G loss: 4.995731]\n",
      "[Epoch 0/200] [Batch 40/938] [D loss: 4.446968] [G loss: 4.995845]\n",
      "[Epoch 0/200] [Batch 41/938] [D loss: 4.446160] [G loss: 4.996553]\n",
      "[Epoch 0/200] [Batch 42/938] [D loss: 4.448821] [G loss: 4.994155]\n",
      "[Epoch 0/200] [Batch 43/938] [D loss: 4.447187] [G loss: 4.995486]\n",
      "[Epoch 0/200] [Batch 44/938] [D loss: 4.446437] [G loss: 4.996365]\n",
      "[Epoch 0/200] [Batch 45/938] [D loss: 4.444843] [G loss: 4.997970]\n",
      "[Epoch 0/200] [Batch 46/938] [D loss: 4.449967] [G loss: 4.992965]\n",
      "[Epoch 0/200] [Batch 47/938] [D loss: 4.446220] [G loss: 4.996500]\n",
      "[Epoch 0/200] [Batch 48/938] [D loss: 4.445595] [G loss: 4.997059]\n",
      "[Epoch 0/200] [Batch 49/938] [D loss: 4.448366] [G loss: 4.994318]\n",
      "[Epoch 0/200] [Batch 50/938] [D loss: 4.448241] [G loss: 4.994432]\n",
      "[Epoch 0/200] [Batch 51/938] [D loss: 4.444832] [G loss: 4.997922]\n",
      "[Epoch 0/200] [Batch 52/938] [D loss: 4.445991] [G loss: 4.996797]\n",
      "[Epoch 0/200] [Batch 53/938] [D loss: 4.449384] [G loss: 4.993368]\n",
      "[Epoch 0/200] [Batch 54/938] [D loss: 4.447417] [G loss: 4.995448]\n",
      "[Epoch 0/200] [Batch 55/938] [D loss: 4.443912] [G loss: 4.998850]\n",
      "[Epoch 0/200] [Batch 56/938] [D loss: 4.450225] [G loss: 4.992536]\n",
      "[Epoch 0/200] [Batch 57/938] [D loss: 4.446342] [G loss: 4.996397]\n",
      "[Epoch 0/200] [Batch 58/938] [D loss: 4.445745] [G loss: 4.996983]\n",
      "[Epoch 0/200] [Batch 59/938] [D loss: 4.447727] [G loss: 4.994957]\n",
      "[Epoch 0/200] [Batch 60/938] [D loss: 4.446795] [G loss: 4.995892]\n",
      "[Epoch 0/200] [Batch 61/938] [D loss: 4.447639] [G loss: 4.995090]\n",
      "[Epoch 0/200] [Batch 62/938] [D loss: 4.445535] [G loss: 4.997228]\n",
      "[Epoch 0/200] [Batch 63/938] [D loss: 4.447205] [G loss: 4.995558]\n",
      "[Epoch 0/200] [Batch 64/938] [D loss: 4.447494] [G loss: 4.995277]\n",
      "[Epoch 0/200] [Batch 65/938] [D loss: 4.446440] [G loss: 4.996212]\n",
      "[Epoch 0/200] [Batch 66/938] [D loss: 4.447701] [G loss: 4.995046]\n",
      "[Epoch 0/200] [Batch 67/938] [D loss: 4.446967] [G loss: 4.995735]\n",
      "[Epoch 0/200] [Batch 68/938] [D loss: 4.445542] [G loss: 4.997134]\n",
      "[Epoch 0/200] [Batch 69/938] [D loss: 4.448732] [G loss: 4.993958]\n",
      "[Epoch 0/200] [Batch 70/938] [D loss: 4.446486] [G loss: 4.996159]\n",
      "[Epoch 0/200] [Batch 71/938] [D loss: 4.445275] [G loss: 4.997456]\n",
      "[Epoch 0/200] [Batch 72/938] [D loss: 4.448061] [G loss: 4.994646]\n",
      "[Epoch 0/200] [Batch 73/938] [D loss: 4.446737] [G loss: 4.995895]\n",
      "[Epoch 0/200] [Batch 74/938] [D loss: 4.447335] [G loss: 4.995333]\n",
      "[Epoch 0/200] [Batch 75/938] [D loss: 4.446460] [G loss: 4.996266]\n",
      "[Epoch 0/200] [Batch 76/938] [D loss: 4.447271] [G loss: 4.995377]\n",
      "[Epoch 0/200] [Batch 77/938] [D loss: 4.446533] [G loss: 4.996055]\n",
      "[Epoch 0/200] [Batch 78/938] [D loss: 4.446541] [G loss: 4.996076]\n",
      "[Epoch 0/200] [Batch 79/938] [D loss: 4.447465] [G loss: 4.995268]\n",
      "[Epoch 0/200] [Batch 80/938] [D loss: 4.446612] [G loss: 4.995997]\n",
      "[Epoch 0/200] [Batch 81/938] [D loss: 4.446910] [G loss: 4.995770]\n",
      "[Epoch 0/200] [Batch 82/938] [D loss: 4.446868] [G loss: 4.995788]\n",
      "[Epoch 0/200] [Batch 83/938] [D loss: 4.447371] [G loss: 4.995302]\n",
      "[Epoch 0/200] [Batch 84/938] [D loss: 4.445689] [G loss: 4.996915]\n",
      "[Epoch 0/200] [Batch 85/938] [D loss: 4.446549] [G loss: 4.996033]\n",
      "[Epoch 0/200] [Batch 86/938] [D loss: 4.447847] [G loss: 4.994814]\n",
      "[Epoch 0/200] [Batch 87/938] [D loss: 4.445430] [G loss: 4.997164]\n",
      "[Epoch 0/200] [Batch 88/938] [D loss: 4.447697] [G loss: 4.994943]\n",
      "[Epoch 0/200] [Batch 89/938] [D loss: 4.447045] [G loss: 4.995659]\n",
      "[Epoch 0/200] [Batch 90/938] [D loss: 4.445931] [G loss: 4.996721]\n",
      "[Epoch 0/200] [Batch 91/938] [D loss: 4.448045] [G loss: 4.994590]\n",
      "[Epoch 0/200] [Batch 92/938] [D loss: 4.445980] [G loss: 4.996739]\n",
      "[Epoch 0/200] [Batch 93/938] [D loss: 4.447409] [G loss: 4.995282]\n",
      "[Epoch 0/200] [Batch 94/938] [D loss: 4.446997] [G loss: 4.995728]\n",
      "[Epoch 0/200] [Batch 95/938] [D loss: 4.445882] [G loss: 4.996770]\n",
      "[Epoch 0/200] [Batch 96/938] [D loss: 4.448160] [G loss: 4.994583]\n",
      "[Epoch 0/200] [Batch 97/938] [D loss: 4.445764] [G loss: 4.996902]\n",
      "[Epoch 0/200] [Batch 98/938] [D loss: 4.448054] [G loss: 4.994540]\n",
      "[Epoch 0/200] [Batch 99/938] [D loss: 4.444455] [G loss: 4.998124]\n",
      "[Epoch 0/200] [Batch 100/938] [D loss: 4.450854] [G loss: 4.991835]\n",
      "[Epoch 0/200] [Batch 101/938] [D loss: 4.443057] [G loss: 4.999639]\n",
      "[Epoch 0/200] [Batch 102/938] [D loss: 4.448868] [G loss: 4.993875]\n",
      "[Epoch 0/200] [Batch 103/938] [D loss: 4.445530] [G loss: 4.997108]\n",
      "[Epoch 0/200] [Batch 104/938] [D loss: 4.448804] [G loss: 4.993852]\n",
      "[Epoch 0/200] [Batch 105/938] [D loss: 4.444360] [G loss: 4.998280]\n",
      "[Epoch 0/200] [Batch 106/938] [D loss: 4.448219] [G loss: 4.994438]\n",
      "[Epoch 0/200] [Batch 107/938] [D loss: 4.447498] [G loss: 4.995144]\n",
      "[Epoch 0/200] [Batch 108/938] [D loss: 4.445340] [G loss: 4.997192]\n",
      "[Epoch 0/200] [Batch 109/938] [D loss: 4.447206] [G loss: 4.995401]\n",
      "[Epoch 0/200] [Batch 110/938] [D loss: 4.448304] [G loss: 4.994354]\n",
      "[Epoch 0/200] [Batch 111/938] [D loss: 4.444589] [G loss: 4.998030]\n",
      "[Epoch 0/200] [Batch 112/938] [D loss: 4.448287] [G loss: 4.994308]\n",
      "[Epoch 0/200] [Batch 113/938] [D loss: 4.446772] [G loss: 4.995802]\n",
      "[Epoch 0/200] [Batch 114/938] [D loss: 4.446844] [G loss: 4.995775]\n",
      "[Epoch 0/200] [Batch 115/938] [D loss: 4.445571] [G loss: 4.997106]\n",
      "[Epoch 0/200] [Batch 116/938] [D loss: 4.448477] [G loss: 4.994123]\n",
      "[Epoch 0/200] [Batch 117/938] [D loss: 4.444575] [G loss: 4.998088]\n",
      "[Epoch 0/200] [Batch 118/938] [D loss: 4.448676] [G loss: 4.993999]\n",
      "[Epoch 0/200] [Batch 119/938] [D loss: 4.446107] [G loss: 4.996461]\n",
      "[Epoch 0/200] [Batch 120/938] [D loss: 4.446035] [G loss: 4.996520]\n",
      "[Epoch 0/200] [Batch 121/938] [D loss: 4.448356] [G loss: 4.994249]\n",
      "[Epoch 0/200] [Batch 122/938] [D loss: 4.444524] [G loss: 4.998099]\n",
      "[Epoch 0/200] [Batch 123/938] [D loss: 4.448582] [G loss: 4.994005]\n",
      "[Epoch 0/200] [Batch 124/938] [D loss: 4.445717] [G loss: 4.996891]\n",
      "[Epoch 0/200] [Batch 125/938] [D loss: 4.447638] [G loss: 4.994912]\n",
      "[Epoch 0/200] [Batch 126/938] [D loss: 4.446794] [G loss: 4.995788]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 127/938] [D loss: 4.445045] [G loss: 4.997534]\n",
      "[Epoch 0/200] [Batch 128/938] [D loss: 4.449203] [G loss: 4.993433]\n",
      "[Epoch 0/200] [Batch 129/938] [D loss: 4.443449] [G loss: 4.999141]\n",
      "[Epoch 0/200] [Batch 130/938] [D loss: 4.450650] [G loss: 4.991965]\n",
      "[Epoch 0/200] [Batch 131/938] [D loss: 4.443278] [G loss: 4.999403]\n",
      "[Epoch 0/200] [Batch 132/938] [D loss: 4.450336] [G loss: 4.992303]\n",
      "[Epoch 0/200] [Batch 133/938] [D loss: 4.444050] [G loss: 4.998502]\n",
      "[Epoch 0/200] [Batch 134/938] [D loss: 4.446909] [G loss: 4.995657]\n",
      "[Epoch 0/200] [Batch 135/938] [D loss: 4.449747] [G loss: 4.992898]\n",
      "[Epoch 0/200] [Batch 136/938] [D loss: 4.441603] [G loss: 5.001083]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8624/1879903528.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0moptimizer_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0moptimizer_D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Pytorch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Pytorch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Pytorch/lib/python3.8/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \"\"\"\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "os.makedirs('Images',exist_ok=True)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=200, help=\"number of epochs of training\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"size of the batches\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\n",
    "parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\n",
    "parser.add_argument(\"--latent_dim\", type=int, default=100, help=\"dimensionality of the latent space\")\n",
    "parser.add_argument(\"--img_size\", type=int, default=28, help=\"size of each image dimension\")\n",
    "parser.add_argument(\"--channels\", type=int, default=1, help=\"number of image channels\")\n",
    "parser.add_argument(\"--sample_interval\", type=int, default=400, help=\"interval betwen image samples\")\n",
    "opt = parser.parse_args(args=[])\n",
    "print('---------------\\n')\n",
    "print(opt)\n",
    "print('---------------\\n')\n",
    "\n",
    "img_shape = (opt.channels, opt.img_size, opt.img_size)\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            *block(opt.latent_dim, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.shape[0], *img_shape)\n",
    "        return img\n",
    "        \n",
    "        \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(opt.img_size**2, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.shape[0], -1)\n",
    "        validity = self.model(img_flat)\n",
    "        return validity\n",
    "    \n",
    "# Loss function\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "    \n",
    "# Configure data loader\n",
    "os.makedirs(\"../../mnist\", exist_ok=True)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        \"../../data/mnist\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize(opt.img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "def log(x):\n",
    "    return torch.log(x+1e-8)\n",
    "\n",
    "\n",
    "'''\n",
    "--------\n",
    "training\n",
    "--------\n",
    "'''\n",
    "\n",
    "for epoch in range(opt.n_epochs):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "        optimizer_G.zero_grad()\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        batch_size = imgs.shape[0]\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        g_targrt = 1/(batch_size*2)\n",
    "        d_target = 1/batch_size\n",
    "        \n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "        \n",
    "        # Sample noise as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim))))\n",
    "        # Generator a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "        \n",
    "        d_real = discriminator(real_imgs)\n",
    "        d_fake = discriminator(gen_imgs)\n",
    "        \n",
    "        # Partition function\n",
    "        Z = torch.sum(torch.exp(-d_real)) + torch.sum(torch.exp(-d_fake))\n",
    "        \n",
    "        # Calcuate loss of discriminator and update\n",
    "        d_loss = d_target * torch.sum(d_real) + log(Z)\n",
    "        d_loss.backward(retain_graph=True)\n",
    "        # optimizer_D.step()\n",
    "        \n",
    "        # Calcuate loss od generator and update\n",
    "        g_loss = g_targrt * (torch.sum(d_real) + torch.sum(d_fake)) + log(Z)\n",
    "        g_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        print(\n",
    "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "            % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
    "        )\n",
    "        \n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % opt.sample_interval == 0:\n",
    "            save_image(gen_imgs.data[:25], \"Images/ryan%d.png\" % batches_done, nrow=5, normalize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
